{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3045 files belonging to 2 classes.\n",
      "Found 16 files belonging to 2 classes.\n",
      "Found 624 files belonging to 2 classes.\n",
      "['NORMAL', 'PNEUMONIA']\n",
      "['NORMAL', 'PNEUMONIA']\n",
      "['NORMAL', 'PNEUMONIA']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "local_dir = Path().cwd()\n",
    "train_dir = local_dir / 'data/train/'\n",
    "test_dir = local_dir / 'data/test/'\n",
    "val_dir = local_dir / 'data/val/'\n",
    "\n",
    "#Reads the data from disk\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(train_dir, image_size=(224, 224), seed=123)\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(val_dir, image_size=(224, 224), seed=123)\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(test_dir, image_size=(224, 224), seed=123)\n",
    "\n",
    "print(train_ds.class_names)\n",
    "print(val_ds.class_names)\n",
    "print(test_ds.class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\", seed=123),\n",
    "    tf.keras.layers.RandomRotation(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "# prints the shape of the training dataset\n",
    "for image_batch, labels_batch in train_ds:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the dataset for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('tfmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "])\n",
    "aug_train_ds = train_ds.map(\n",
    "  lambda x, y: (data_augmentation(x, training=True), y),\n",
    "  num_parallel_calls=AUTOTUNE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "96/96 [==============================] - 398s 4s/step - loss: 0.4378 - accuracy: 0.8634 - val_loss: 0.1387 - val_accuracy: 0.8750\n",
      "Epoch 2/40\n",
      "96/96 [==============================] - 396s 4s/step - loss: 0.1897 - accuracy: 0.9327 - val_loss: 0.1173 - val_accuracy: 0.9375\n",
      "Epoch 3/40\n",
      "96/96 [==============================] - 390s 4s/step - loss: 0.1622 - accuracy: 0.9402 - val_loss: 0.0595 - val_accuracy: 1.0000\n",
      "Epoch 4/40\n",
      "96/96 [==============================] - 398s 4s/step - loss: 0.1610 - accuracy: 0.9389 - val_loss: 0.1443 - val_accuracy: 0.9375\n",
      "Epoch 5/40\n",
      "96/96 [==============================] - 399s 4s/step - loss: 0.1538 - accuracy: 0.9471 - val_loss: 0.0823 - val_accuracy: 0.9375\n",
      "Epoch 6/40\n",
      "96/96 [==============================] - 394s 4s/step - loss: 0.1254 - accuracy: 0.9521 - val_loss: 0.0502 - val_accuracy: 1.0000\n",
      "Epoch 7/40\n",
      "96/96 [==============================] - 389s 4s/step - loss: 0.1190 - accuracy: 0.9530 - val_loss: 0.1558 - val_accuracy: 0.9375\n",
      "Epoch 8/40\n",
      "96/96 [==============================] - 368s 4s/step - loss: 0.1142 - accuracy: 0.9593 - val_loss: 0.2656 - val_accuracy: 0.8750\n",
      "Epoch 9/40\n",
      "96/96 [==============================] - 368s 4s/step - loss: 0.1087 - accuracy: 0.9655 - val_loss: 0.0785 - val_accuracy: 1.0000\n",
      "Epoch 10/40\n",
      "96/96 [==============================] - 368s 4s/step - loss: 0.1039 - accuracy: 0.9626 - val_loss: 0.2474 - val_accuracy: 0.8750\n",
      "Epoch 11/40\n",
      "96/96 [==============================] - 367s 4s/step - loss: 0.1030 - accuracy: 0.9645 - val_loss: 0.0894 - val_accuracy: 1.0000\n",
      "Epoch 12/40\n",
      "96/96 [==============================] - 365s 4s/step - loss: 0.1063 - accuracy: 0.9576 - val_loss: 0.0941 - val_accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "96/96 [==============================] - 366s 4s/step - loss: 0.0892 - accuracy: 0.9681 - val_loss: 0.1947 - val_accuracy: 0.8750\n",
      "Epoch 14/40\n",
      "96/96 [==============================] - 367s 4s/step - loss: 0.0932 - accuracy: 0.9652 - val_loss: 0.1362 - val_accuracy: 0.9375\n",
      "Epoch 15/40\n",
      "96/96 [==============================] - 370s 4s/step - loss: 0.0830 - accuracy: 0.9665 - val_loss: 0.0810 - val_accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "96/96 [==============================] - 366s 4s/step - loss: 0.0831 - accuracy: 0.9685 - val_loss: 0.1150 - val_accuracy: 0.9375\n",
      "Epoch 17/40\n",
      "96/96 [==============================] - 368s 4s/step - loss: 0.0733 - accuracy: 0.9754 - val_loss: 0.1108 - val_accuracy: 0.9375\n",
      "Epoch 18/40\n",
      "96/96 [==============================] - 366s 4s/step - loss: 0.0806 - accuracy: 0.9718 - val_loss: 0.0420 - val_accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "96/96 [==============================] - 365s 4s/step - loss: 0.0962 - accuracy: 0.9655 - val_loss: 0.0582 - val_accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "96/96 [==============================] - 368s 4s/step - loss: 0.0746 - accuracy: 0.9747 - val_loss: 0.0177 - val_accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "96/96 [==============================] - 369s 4s/step - loss: 0.0792 - accuracy: 0.9695 - val_loss: 0.0501 - val_accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "96/96 [==============================] - 365s 4s/step - loss: 0.0700 - accuracy: 0.9754 - val_loss: 0.0934 - val_accuracy: 0.9375\n",
      "Epoch 23/40\n",
      "96/96 [==============================] - 368s 4s/step - loss: 0.0835 - accuracy: 0.9668 - val_loss: 0.0486 - val_accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "96/96 [==============================] - 368s 4s/step - loss: 0.0715 - accuracy: 0.9747 - val_loss: 0.1265 - val_accuracy: 0.9375\n",
      "Epoch 25/40\n",
      "96/96 [==============================] - 367s 4s/step - loss: 0.0826 - accuracy: 0.9685 - val_loss: 0.0568 - val_accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "96/96 [==============================] - 368s 4s/step - loss: 0.0720 - accuracy: 0.9731 - val_loss: 0.0888 - val_accuracy: 0.9375\n",
      "Epoch 27/40\n",
      "96/96 [==============================] - 368s 4s/step - loss: 0.0641 - accuracy: 0.9770 - val_loss: 0.0713 - val_accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "96/96 [==============================] - 368s 4s/step - loss: 0.0595 - accuracy: 0.9783 - val_loss: 0.0265 - val_accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "96/96 [==============================] - 368s 4s/step - loss: 0.0708 - accuracy: 0.9754 - val_loss: 0.0636 - val_accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "96/96 [==============================] - 365s 4s/step - loss: 0.0697 - accuracy: 0.9734 - val_loss: 0.2530 - val_accuracy: 0.8125\n",
      "Epoch 31/40\n",
      "96/96 [==============================] - 366s 4s/step - loss: 0.0695 - accuracy: 0.9731 - val_loss: 0.0175 - val_accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "96/96 [==============================] - 365s 4s/step - loss: 0.0530 - accuracy: 0.9826 - val_loss: 0.1724 - val_accuracy: 0.9375\n",
      "Epoch 33/40\n",
      "96/96 [==============================] - 365s 4s/step - loss: 0.0598 - accuracy: 0.9803 - val_loss: 0.1748 - val_accuracy: 0.9375\n",
      "Epoch 34/40\n",
      "96/96 [==============================] - 368s 4s/step - loss: 0.0562 - accuracy: 0.9783 - val_loss: 0.0379 - val_accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "96/96 [==============================] - 367s 4s/step - loss: 0.0550 - accuracy: 0.9796 - val_loss: 0.0397 - val_accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "96/96 [==============================] - 366s 4s/step - loss: 0.0721 - accuracy: 0.9737 - val_loss: 0.0566 - val_accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "96/96 [==============================] - 367s 4s/step - loss: 0.0503 - accuracy: 0.9823 - val_loss: 0.0318 - val_accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "96/96 [==============================] - 367s 4s/step - loss: 0.0592 - accuracy: 0.9816 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "96/96 [==============================] - 367s 4s/step - loss: 0.0590 - accuracy: 0.9813 - val_loss: 0.0240 - val_accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "96/96 [==============================] - 367s 4s/step - loss: 0.0561 - accuracy: 0.9816 - val_loss: 0.0948 - val_accuracy: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d949602800>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the network\n",
    "model.fit(\n",
    "  aug_train_ds,\n",
    "  validation_data=val_ds,\n",
    "  batch_size=64,\n",
    "  epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 20s 941ms/step - loss: 0.8558 - accuracy: 0.8365\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8558077812194824, 0.8365384340286255]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tfmodel_2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tfmodel_2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('tfmodel_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6249b961082d366c7aea2f36fb098140763d015244db7e7b3bc91203ca6bf9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
